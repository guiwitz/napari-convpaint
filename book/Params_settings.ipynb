{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a465dc8",
   "metadata": {},
   "source": [
    "# Parameters and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558e4f6",
   "metadata": {},
   "source": [
    "In this section, the parameters and settings available in Convpaint are described.\n",
    "\n",
    "While parameters directly influence the classification process, settings allow you to adjust the behavior of the plugin or API without affecting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3846e",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "364c419c",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"doc doc-object doc-class\">\n",
       "<a id=\"napari_convpaint.conv_paint_param.Param\"></a>\n",
       "<div class=\"doc doc-contents first\">\n",
       "<p>The Param object organizes the <strong>parameters that control the behavior of the feature extraction and classification processes</strong>.</p>\n",
       "<p>It therefore defines the <strong>processing and results</strong> of the ConvpaintModel.</p>\n",
       "<p>These parameters can be adjusted to optimize the <strong>performance</strong> of the model for specific use cases.</p>\n",
       "<p><span class=\"doc-section-title\">Parameters:</span></p>\n",
       "<table>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>Name</th>\n",
       "<th>Type</th>\n",
       "<th>Description</th>\n",
       "<th>Default</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>classifier</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"str\">str</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Path to the classifier model (if saved, otherwise None)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>multi_channel_img</code>\n",
       "</td>\n",
       "<td>\n",
       "<code>bool = None</code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Interpret the first dimension as channels (as opposed to z or time)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>normalize</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Normalization mode:\n",
       "    1 = no normalization,\n",
       "    2 = normalize stack,\n",
       "    3 = normalize each image</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>image_downsample</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Factor for downscaling the image right after input\n",
       "(predicted classes are upsampled accordingly for output)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>seg_smoothening</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Factor for smoothening the segmentation output with a Majority filter</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>tile_annotations</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>If True, extract only features of bounding boxes around annotated areas when training</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>tile_image</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>If True, extract features in tiles when running predictions (for large images)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>use_dask</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>If True, use dask for parallel processing (currently only used when tiling images)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>unpatch_order</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Order of interpolation for unpatching the output of patch-based FEs (default = 1 = bilinear interpolation)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_name</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"str\">str</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Name of the feature extractor model</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_layers</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"list\">list</span>[<span title=\"str\">str</span>]</code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>List of layers (names or indices among available layers) to extract features from</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_use_gpu</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Whether to use GPU for feature extraction</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_scalings</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"list\">list</span>[<span title=\"int\">int</span>]</code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>List of scaling factors for the feature extractor, creating a pyramid of features\n",
       "(features are rescaled accordingly before input to classifier)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_order</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Interpolation order used for the upscaling of features of the pyramid</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_use_min_features</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>If True, use the minimum number of features among all layers (simply taking the first x features)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>clf_iterations</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Number of iterations for the classifier</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>clf_learning_rate</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"float\">float</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Learning rate for the classifier</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>clf_depth</code>\n",
       "</td>\n",
       "<td>\n",
       "<code>int = None</code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Depth of the classifier</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>clf_use_gpu</code>\n",
       "</td>\n",
       "<td>\n",
       "<code>bool = None</code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Whether to use GPU for the classifier\n",
       "(if None, fe_use_gpu is used)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<div class=\"doc doc-children\">\n",
       "</div>\n",
       "</div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"../site/Param/index.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "# grab only the class doc container\n",
    "doc_div = soup.find(\"div\", class_=\"doc doc-object doc-class\")\n",
    "html_fragment = str(doc_div)\n",
    "\n",
    "# Display in Jupyter\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(html_fragment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9b2a6",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144b104",
   "metadata": {},
   "source": [
    "Besides the parameters that directly influence the classification process, Convpaint contains options which **should not affect the results** of the classification, but let you **adjust the behaviour** of the plugin or API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279dbf7",
   "metadata": {},
   "source": [
    "### \"Auto segment\" (plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f139f",
   "metadata": {},
   "source": [
    "Often we want to inspect the results of segmentation right away on the image that we used for training. For this purpose, we added an option to the plugin that let's you **segment the image automatically after training** is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b132a",
   "metadata": {},
   "source": [
    "### Layers handling (plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d45aac8",
   "metadata": {},
   "source": [
    "We let you adjust the **behaviour of the annotation layers** in the *Advanced* tab of the plugin settings. By default, an annotation layer is automatically added whenever you select a new image in Convpaint, overwriting any existing annotation layers. You can change this behaviour in two ways:\n",
    "\n",
    "- Turn off the automatic addition of annotation layers; in this case, you need to manually add annotation layers after selecting a new image, e.g. through the button \"Add annotation layer\" in the plugin.\n",
    "- Tick \"Keep old layers\", in order to backup existing annotation layers and prevent them from being overwritten; the backup layers will be renamed according to the selected image and chosen image type (e.g. multichannel).\n",
    "\n",
    "We provide two **more options** for handling annotation layers:\n",
    "\n",
    "- Clicking \"Add for all selected\" will add annotation layers for all images which are selected in the napari layer list (typically on the left side); the layers are named as described above for the backup layers.\n",
    "- Ticking \"Auto-select annotation layer\" will automatically select the annotation layer corresponding to the currently selected image, given they are named according to the conventions described."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79b575",
   "metadata": {},
   "source": [
    "### memory_mode (API) and \"Continuous training\" (plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba3aefe",
   "metadata": {},
   "source": [
    "Often times we're not done after one round of annotating and training. For this purpose, we added an option that saves the annotations as well as the according features inside the `ConvpaintModel` instance. Like this, it is even possible to iteratively combine features from different images.\n",
    "\n",
    "In the **API**, this behaviour is controlled by the `memory_mode` parameter of the `train()` method (as well as the `get_feature_image()` method). If set to `True`, the model will retain all annotations and features across training sessions, allowing for iterative training and refinement. If using across images, image_ids must be provided, in order to differentiate between the images.\n",
    "\n",
    "If `memory_mode` is turned on, features will be both *loaded* from and *saved* into the ConvpaintModel instance. If turned off, the saved features will be ignored. However, they will only be discarded when manually resetting either the training features (`reset_training()`) or the classifier (`reset_classifier()`, which internally calls `reset_features()`).\n",
    "\n",
    "In the **plugin**, the `Continuous training` option can be adjusted to allow for this behavior. When enabled, the plugin will run the underlying methods using `memory_mode=True` and automatically save all annotations and features after each training session. With the option `Image`, the training is reset whenever you switch to a different image, giving the advantage of not having to extract features again. If the option `Global` is selected, the model will retain all annotations and features across different images. When turned `Off`, the model will neither load nor save any annotations or features.\n",
    "\n",
    "*Sidenote: In the plugin, the default option is `Image`. Turn it off, if this is not desired.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debd7ff",
   "metadata": {},
   "source": [
    "### use_dask (API and plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520075e",
   "metadata": {},
   "source": [
    "We allow users to **use Dask for parallel computing**, which can significantly speed up the training process, especially for large datasets. Currently, this is **still a beta feature** and may not yet be fully optimized. Additionally, Dask is only available for parallelizing predictions across multiple tiles when image tiling for prediction is enabled.\n",
    "\n",
    "To use Dask, you can set the `use_dask` parameter to `True` in the **API** (`segment()`and `predict_probas()`methods) or enable the corresponding option in the *Advanced* tab of the **plugin** settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119a985",
   "metadata": {},
   "source": [
    "### in_channels (API and plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78246224",
   "metadata": {},
   "source": [
    "Sometimes you want to use a **subset of input channels** and maybe even compare channels between images that are in different orders. For this, we let you specify the `in_channels` parameter in the **API** (any method that takes image inputs) or the corresponding option in the *Advanced* tab of the **plugin** settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b88d0",
   "metadata": {},
   "source": [
    "### Predicting probabilities (API and plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df89e75",
   "metadata": {},
   "source": [
    "Besides getting a segmentation as output, you can also obtain **per-pixel probabilities for each class**. This can be useful for tasks where you need to know the uncertainty of the predictions or when you want to apply custom post-processing steps based on the confidence of the model.\n",
    "\n",
    "To get the predicted probabilities, you can use the `predict_probas()` methods in the **API** or tick the corresponding output option in the *Advanced* tab of the **plugin** settings.\n",
    "\n",
    "*Sidenote: For segmentation, Convpaint internally calculates the predicted probabilities for each class and determines the most probable class to generate the final segmentation mask.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp-env02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
