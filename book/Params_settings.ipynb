{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a465dc8",
   "metadata": {},
   "source": [
    "# Parameters and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558e4f6",
   "metadata": {},
   "source": [
    "In this section, the parameters and settings available in Convpaint are described. For a comprehensive description of the core `ConvpaintModel` class, please refer to the separate [page](https://guiwitz.github.io/napari-convpaint/book/ConvpaintModel.html).\n",
    "\n",
    "While parameters directly influence the classification process, settings allow you to adjust the behavior of the plugin or API without affecting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3846e",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364c419c",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f14e772",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "fix_css = \"\"\"\n",
    "<style>\n",
    ".doc.doc-object.doc-class {\n",
    "    background: transparent !important;\n",
    "    border: none !important;\n",
    "    box-shadow: none !important;\n",
    "}\n",
    "# .doc.doc-object.doc-class code {\n",
    "#     background-color: #f5f5f5; /* softer background */\n",
    "#     color: #030303;           /* example code color */\n",
    "#     padding: 2px 4px;\n",
    "#     border-radius: 4px;\n",
    "# }\n",
    "</style>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b8a5d0",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".doc.doc-object.doc-class {\n",
       "    background: transparent !important;\n",
       "    border: none !important;\n",
       "    box-shadow: none !important;\n",
       "}\n",
       "# .doc.doc-object.doc-class code {\n",
       "#     background-color: #f5f5f5; /* softer background */\n",
       "#     color: #030303;           /* example code color */\n",
       "#     padding: 2px 4px;\n",
       "#     border-radius: 4px;\n",
       "# }\n",
       "</style>\n",
       "<div class=\"doc doc-object doc-class\">\n",
       "<a id=\"napari_convpaint.conv_paint_param.Param\"></a>\n",
       "<div class=\"doc doc-contents first\">\n",
       "<p>The <code>Param</code> object organizes the <strong>parameters that control the behavior of the feature extraction and classification processes</strong>.</p>\n",
       "<p>It therefore defines the <strong>processing and results</strong> with Convpaint.</p>\n",
       "<p>These parameters can be adjusted to optimize the <strong>performance</strong> of the model for specific use cases.</p>\n",
       "<p><span class=\"doc-section-title\">Parameters:</span></p>\n",
       "<table>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>Name</th>\n",
       "<th>Type</th>\n",
       "<th>Description</th>\n",
       "<th>Default</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>classifier</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"str\">str</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Path to the classifier model (if saved, otherwise None)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>channel_mode</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"str\">str</span>('multi', 'rgb', 'single')</code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>\"multi\": interpret the first dimension as channels (as opposed to z or time);\n",
       "\"rgb\": interpret the first dimension as rgb channels (3 channels required), important for some FEs;\n",
       "\"single\": interpret the first dimension as z or time (only one channel);\n",
       "Note: if data is given as 4d array, \"single\" is not valid and will be changed to \"multi\"</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>'single'</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>normalize</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Normalization mode:\n",
       "    1 = no normalization;\n",
       "    2 = normalize stack;\n",
       "    3 = normalize each image</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>image_downsample</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Factor for downscaling the image right after input\n",
       "(predicted classes are upsampled accordingly for output).\n",
       "Hint: use negative numbers for upsampling instead.</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>seg_smoothening</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Factor for smoothening the segmentation output with a Majority filter</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>tile_annotations</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>If True, extract only features of bounding boxes around annotated areas when training</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>tile_image</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>If True, extract features in tiles when running predictions (for large images)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>use_dask</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>If True, use dask for parallel processing (currently only used when tiling images)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>unpatch_order</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Order of interpolation for unpatching the output of patch-based FEs (default = 1 = bilinear interpolation)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_name</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"str\">str</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Name of the feature extractor model</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_layers</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"list\">list</span>[<span title=\"str\">str</span>]</code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>List of layers (names or indices among available layers) to extract features from</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_use_gpu</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Whether to use GPU for feature extraction</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_scalings</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"list\">list</span>[<span title=\"int\">int</span>]</code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>List of scaling factors for the feature extractor, creating a pyramid of features\n",
       "(features are rescaled accordingly before input to classifier)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_order</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Interpolation order used for the upscaling of features of the pyramid</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>fe_use_min_features</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>If True, use the minimum number of features among all layers (simply taking the first x features)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>clf_iterations</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Number of iterations for the classifier</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>clf_learning_rate</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"float\">float</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Learning rate for the classifier</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>clf_depth</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"int\">int</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Depth of the classifier</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr class=\"doc-section-item\">\n",
       "<td>\n",
       "<code>clf_use_gpu</code>\n",
       "</td>\n",
       "<td>\n",
       "<code><span title=\"bool\">bool</span></code>\n",
       "</td>\n",
       "<td>\n",
       "<div class=\"doc-md-description\">\n",
       "<p>Whether to use GPU for the classifier\n",
       "(if None, fe_use_gpu is used)</p>\n",
       "</div>\n",
       "</td>\n",
       "<td>\n",
       "<code>None</code>\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<div class=\"doc doc-children\">\n",
       "</div>\n",
       "</div>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"../site/Param/index.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "# grab only the class doc container\n",
    "doc_div = soup.find(\"div\", class_=\"doc doc-object doc-class\")\n",
    "html_fragment = str(doc_div)\n",
    "\n",
    "# Display in Jupyter\n",
    "display(HTML(fix_css + html_fragment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9b2a6",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144b104",
   "metadata": {},
   "source": [
    "Besides the parameters that directly influence the classification process, Convpaint contains options which **should not affect the results** of the classification, but let you **adjust the behaviour** of the plugin or API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279dbf7",
   "metadata": {},
   "source": [
    "### \"Auto segment\" (plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f139f",
   "metadata": {},
   "source": [
    "Often we want to inspect the results of segmentation right away on the image that we used for training. For this purpose, we added an option to the plugin that let's you **segment the image automatically after training** is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b132a",
   "metadata": {},
   "source": [
    "### Layers handling (plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d45aac8",
   "metadata": {},
   "source": [
    "We let you adjust the **behaviour of the annotation layers** in the *Advanced* tab of the plugin settings. By default, an annotation layer is automatically added whenever you select a new image in Convpaint, overwriting any existing annotation layers. You can change this behaviour in two ways:\n",
    "\n",
    "- Turn off the automatic addition of annotation layers; in this case, you need to manually add annotation layers after selecting a new image, e.g. through the button \"Add annotation layer\" in the plugin.\n",
    "- Tick \"Keep old layers\", in order to backup existing annotation layers and prevent them from being overwritten; the backup layers will be renamed according to the selected image and chosen image type (e.g. multichannel).\n",
    "\n",
    "We provide two **more options** for handling annotation layers:\n",
    "\n",
    "- Clicking \"Add for all selected\" will add annotation layers for all images which are selected in the napari layer list (typically on the left side); the layers are named as described above for the backup layers.\n",
    "- Ticking \"Auto-select annotation layer\" will automatically select the annotation layer corresponding to the currently selected image, given they are named according to the conventions described."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79b575",
   "metadata": {},
   "source": [
    "### memory_mode (API) and \"Continuous training\" (plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba3aefe",
   "metadata": {},
   "source": [
    "Often times we're not done after one round of annotating and training. For this purpose, we added an option that saves the annotations as well as the according features inside the `ConvpaintModel` instance. This **avoids extracting features from the same pixels** again, which can save time especially when we use image tiling for training. And it even enables to iteratively **combine features from different images**.\n",
    "\n",
    "In the **API**, this behaviour is controlled by the `memory_mode` parameter of the `train()` method (as well as the `get_feature_image()` method). If set to `True`, the model will retain all annotations and features across training sessions, allowing for iterative training and refinement. If using it across images, image_ids must be provided, in order to differentiate between the images.\n",
    "\n",
    "If `memory_mode` is turned on, features will be both *loaded from* and *saved into* the ConvpaintModel instance. If turned off, the saved features will be ignored. However, they will only be discarded when manually resetting either the training features (`reset_training()`) or the classifier (`reset_classifier()`, which internally calls `reset_features()`).\n",
    "\n",
    "In the **plugin**, the `Continuous training` option can be adjusted to allow for this behavior. When enabled, the plugin will run the underlying methods using `memory_mode=True` and automatically save all annotations and features after each training session.\n",
    "\n",
    "With the option `Image`, the training is reset whenever you switch to a different image, giving the advantage of not having to extract features again. If the option `Global` is selected, the model will retain all annotations and features across different images. When turned `Off`, the model will neither load nor save any annotations or features.\n",
    "\n",
    "In the plugin, the **default option is `Image`**. Turn it off, if this is not desired.\n",
    "\n",
    "*Sidenote: Using an image downsampled at different scales will create separate feature entries for each scale (as if it were different images, but without resetting the training in between if `memory_mode` is set to `Image`).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debd7ff",
   "metadata": {},
   "source": [
    "### use_dask (API and plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520075e",
   "metadata": {},
   "source": [
    "We allow users to **use Dask for parallel computing**, which can significantly speed up the training process, especially for large datasets. Currently, this is **still a beta feature** and may not yet be fully optimized. Additionally, Dask is only available for parallelizing predictions across multiple tiles when image tiling for prediction is enabled.\n",
    "\n",
    "To use Dask, you can set the `use_dask` parameter to `True` in the **API** (`segment()`and `predict_probas()`methods) or enable the corresponding option in the *Advanced* tab of the **plugin** settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119a985",
   "metadata": {},
   "source": [
    "### in_channels (API and plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78246224",
   "metadata": {},
   "source": [
    "Sometimes you want to use a **subset of input channels** and maybe even compare channels between images that are in different orders. For this, we let you specify the `in_channels` parameter in the **API** (any method that takes image inputs) or the corresponding option in the *Advanced* tab of the **plugin** settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b88d0",
   "metadata": {},
   "source": [
    "### Predicting probabilities (API and plugin, *Advanced* tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df89e75",
   "metadata": {},
   "source": [
    "Besides getting a segmentation as output, you can also obtain **per-pixel probabilities for each class**. This can be useful for tasks where you need to know the uncertainty of the predictions or when you want to apply custom post-processing steps based on the confidence of the model.\n",
    "\n",
    "To get the predicted probabilities, you can use the `predict_probas()` methods in the **API** or tick the corresponding output option in the *Advanced* tab of the **plugin** settings.\n",
    "\n",
    "*Sidenote: For segmentation, Convpaint internally calculates the predicted probabilities for each class and determines the most probable class to generate the final segmentation mask.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp-env02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
